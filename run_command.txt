python train.py --dataset fineweb --model_path meta-llama/Llama-3.2-1B --target_layer 8 --loss_type kl_divergence --kl_temperature 1.0 --ft_head --batch_size 4 --grad_accumulate_steps 4 --max_steps 200 --learning_rate 2e-5 --warmup_step_ratio 0.1 --max_length 4096 --device cuda --output_dir ../checkpoints/models --run_type dryrun_h100 --notes "H100-80GB dryrun: L3.2-1B, layer8, KL, head-only, FineWeb" --wandb --wandb_project "Resurrection"